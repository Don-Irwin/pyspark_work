{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62292812-f394-447c-beaf-8b7d71d15a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from external.db_and_excel_utilities.utility import Utility as mutil\n",
    "from external.db_and_excel_utilities.db_base import db_base\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b405a0-a341-4fc9-bd1e-65f0e48a8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = mutil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c5c7c5-2d32-4683-b8b4-83cb17b15407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/21 17:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/21 17:16:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "| id|        product_name|product_type_id|          created_by|          created_dt|updated_by|updated_dt|parent_product_id|\n",
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "|  1|Melissa's Crazy C...|             11|sa               ...|2019-07-05 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  2| William's Great War|              8|sa               ...|2018-12-09 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  3|Kevin's Spy Thriller|              8|sa               ...|2019-01-20 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  4|Lisa's Patriotic ...|              2|sa               ...|2020-06-22 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  5|  Ryan's Big Romance|              1|sa               ...|2020-06-20 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  6|Jesus's Monster M...|             10|sa               ...|2020-11-21 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  7|Elizabeth's Trip ...|             11|sa               ...|2020-04-16 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  8|Jennifer's Spy Th...|              6|sa               ...|2019-03-25 08:28:...|      NULL|      NULL|             NULL|\n",
      "|  9|Marissa's Patriot...|              3|sa               ...|2020-02-18 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 10|Deanna's Monster ...|              1|sa               ...|2019-12-12 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 11|George's Crazy Co...|              2|sa               ...|2020-11-17 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 12|Eric's Patriotic ...|              5|sa               ...|2019-05-06 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 13|Shannon's Amazing...|              5|sa               ...|2019-09-10 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 14|Danielle's Big Ro...|              1|sa               ...|2020-09-05 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 15|Colton's Trip in ...|             10|sa               ...|2019-08-03 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 16|Jessica's Monster...|              5|sa               ...|2019-06-23 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 17|    Lisa's Great War|              7|sa               ...|2019-01-19 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 18|Matthew's Spy Thr...|              7|sa               ...|2020-05-25 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 19|Robert's Trip in ...|              9|sa               ...|2020-01-20 08:28:...|      NULL|      NULL|             NULL|\n",
      "| 20|Stephen's Spy Thr...|              3|sa               ...|2020-05-14 08:28:...|      NULL|      NULL|             NULL|\n",
      "+---+--------------------+---------------+--------------------+--------------------+----------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 3 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MSSQLConnectionExample\") \\\n",
    "        .config(\"spark.jars\", \"mssql-jdbc-12.4.2.jre11.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:sqlserver://mssql1:1433;databaseName=products\"\n",
    "    database_properties = {\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "\n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"products\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dced2979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/21 17:16:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "| id|product_id|customer_id|account_id|          post_date|amt_usd|geo_geography_id|fin_distro_channel_id|fin_distro_partner_id|created_by|          created_dt|updated_by|updated_dt|\n",
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "|  1|       437|          1|       119|2023-01-25 08:19:17| $10.92|           35059|                   31|                  403|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  2|       125|          1|       294|2023-02-07 18:15:41| $28.38|           35059|                    9|                  358|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  3|       331|          1|       795|2023-05-13 14:11:26| $14.77|           35059|                   27|                   99|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  4|       371|          2|       287|2023-09-24 02:03:04| $22.28|           35059|                    8|                  691|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  5|        41|          2|       481|2023-03-01 11:13:50| $31.38|           35059|                   20|                  689|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  6|       421|          3|       301|2023-04-09 10:58:41| $13.07|           35056|                    5|                  257|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  7|        22|          3|       481|2023-06-09 16:31:20| $30.37|           35056|                   16|                  511|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  8|        83|          3|       104|2023-05-08 18:51:05| $22.89|           35056|                   18|                   12|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "|  9|       358|          4|       792|2023-05-02 20:37:29| $34.59|           35054|                   13|                   87|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 10|        95|          4|       418|2023-03-14 16:40:42|  $3.58|           35054|                   30|                  183|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 11|       112|          4|       741|2023-10-25 09:06:36| $23.83|           35054|                    7|                  287|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 12|       108|          5|       139|2023-06-30 08:34:50| $25.54|           35050|                    6|                  178|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 13|       428|          5|       787|2023-03-24 18:05:19| $23.10|           35050|                   14|                  495|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 14|         1|          5|       396|2023-02-11 14:54:12| $27.90|           35050|                   11|                  203|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 15|       210|          6|       674|2023-05-25 13:05:36| $24.07|           35049|                    9|                  490|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 16|       261|          6|       150|2023-02-18 08:50:18| $14.57|           35049|                   17|                   26|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 17|       477|          7|       608|2023-02-05 05:54:55|  $3.47|           35045|                   18|                  179|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 18|       332|          7|       656|2023-06-23 06:24:27| $14.90|           35045|                   26|                  177|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 19|       447|          8|       799|2023-11-12 22:55:00|  $9.71|           35045|                    9|                  775|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "| 20|       151|          8|       493|2023-01-27 16:52:59| $28.71|           35045|                    6|                  639|      NULL|2023-11-21 16:28:...|      NULL|      NULL|\n",
      "+---+----------+-----------+----------+-------------------+-------+----------------+---------------------+---------------------+----------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"postgresql.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:postgresql://postsql1:5432/finance\"\n",
    "    database_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"fin_account_activity\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024d48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/21 17:16:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "| id|              f_name|              l_name|       email_address|             country|          postalcode|           city_name|            province|          created_by|         created_dt|updated_by|updated_dt|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "|  1|Connie           ...|Cole             ...|connie.cole@irwin...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2021-02-18 09:12:38|      NULL|      NULL|\n",
      "|  2|Cindy            ...|Smith            ...|cindy.smith@irwin...|USA              ...|77099            ...|Houston          ...|TX               ...|root@%           ...|2022-05-24 23:24:08|      NULL|      NULL|\n",
      "|  3|Harold           ...|Hernandez        ...|harold.hernandez@...|USA              ...|77095            ...|Houston          ...|TX               ...|root@%           ...|2019-09-22 10:22:47|      NULL|      NULL|\n",
      "|  4|Tami             ...|Taylor           ...|tami.taylor@irwin...|USA              ...|77093            ...|Houston          ...|TX               ...|root@%           ...|2022-11-24 02:23:13|      NULL|      NULL|\n",
      "|  5|Amber            ...|Lara             ...|amber.lara@irwina...|USA              ...|77089            ...|Houston          ...|TX               ...|root@%           ...|2021-07-25 19:17:08|      NULL|      NULL|\n",
      "|  6|Natalie          ...|Schroeder        ...|natalie.schroeder...|USA              ...|77088            ...|Houston          ...|TX               ...|root@%           ...|2019-04-17 13:35:49|      NULL|      NULL|\n",
      "|  7|Edward           ...|Holmes           ...|edward.holmes@irw...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2022-12-01 04:25:17|      NULL|      NULL|\n",
      "|  8|Kelsey           ...|Hall             ...|kelsey.hall@irwin...|USA              ...|77084            ...|Houston          ...|TX               ...|root@%           ...|2023-03-08 05:48:02|      NULL|      NULL|\n",
      "|  9|Gina             ...|West             ...|gina.west@irwinan...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2019-02-07 01:46:49|      NULL|      NULL|\n",
      "| 10|Jason            ...|Robinson         ...|jason.robinson@ir...|USA              ...|77083            ...|Houston          ...|TX               ...|root@%           ...|2023-08-07 13:12:40|      NULL|      NULL|\n",
      "| 11|Martin           ...|Smith            ...|martin.smith@irwi...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2020-03-23 20:41:49|      NULL|      NULL|\n",
      "| 12|Deborah          ...|Walker           ...|deborah.walker@ir...|USA              ...|77082            ...|Houston          ...|TX               ...|root@%           ...|2020-11-23 13:16:09|      NULL|      NULL|\n",
      "| 13|Joanne           ...|Shelton          ...|joanne.shelton@ir...|USA              ...|77081            ...|Houston          ...|TX               ...|root@%           ...|2020-03-20 17:15:57|      NULL|      NULL|\n",
      "| 14|Patricia         ...|Pacheco          ...|patricia.pacheco@...|USA              ...|77080            ...|Houston          ...|TX               ...|root@%           ...|2023-02-25 22:05:11|      NULL|      NULL|\n",
      "| 15|Lynn             ...|Park             ...|lynn.park@irwinan...|USA              ...|77077            ...|Houston          ...|TX               ...|root@%           ...|2019-02-22 07:09:58|      NULL|      NULL|\n",
      "| 16|Sean             ...|Castro           ...|sean.castro@irwin...|USA              ...|77073            ...|Houston          ...|TX               ...|root@%           ...|2020-01-07 11:12:16|      NULL|      NULL|\n",
      "| 17|Donna            ...|Conway           ...|donna.conway@irwi...|USA              ...|77072            ...|Houston          ...|TX               ...|root@%           ...|2019-11-06 20:25:24|      NULL|      NULL|\n",
      "| 18|Jeffrey          ...|Beck             ...|jeffrey.beck@irwi...|USA              ...|77070            ...|Houston          ...|TX               ...|root@%           ...|2019-02-01 18:37:10|      NULL|      NULL|\n",
      "| 19|Allison          ...|Oliver           ...|allison.oliver@ir...|USA              ...|77064            ...|Houston          ...|TX               ...|root@%           ...|2021-02-02 17:06:12|      NULL|      NULL|\n",
      "| 20|Matthew          ...|Miranda          ...|matthew.miranda@i...|USA              ...|77060            ...|Houston          ...|TX               ...|root@%           ...|2023-03-19 15:55:16|      NULL|      NULL|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def split_file_and_save_parts():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"SplitFile\").getOrCreate()\n",
    "    \n",
    "    #Location of database drivers\n",
    "    #postgres\n",
    "    #/usr/share/java/postgresql.jar\n",
    "    #mysql\n",
    "    #/usr/share/java/mysql-connector-j-8.2.0.jar\n",
    "    #mssql\n",
    "    #/usr/share/java/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSqlExample\") \\\n",
    "        .config(\"spark.jars\", \"mysql-connector-j-8.2.0.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection properties\n",
    "    database_url = \"jdbc:mysql://mysql1:3306/customers\"\n",
    "    database_properties = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"Python2028\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "        \"encrypt\": \"true\",\n",
    "        \"trustServerCertificate\": \"true\"  # Add this line        \n",
    "    }\n",
    "    \n",
    "    # Read data from MSSQL\n",
    "    df = spark.read.jdbc(url=database_url, table=\"customer_info\", properties=database_properties)\n",
    "\n",
    "    # Show the data\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()    \n",
    "    \n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    \n",
    "    print(f\"Time elapsed: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "# Call the function\n",
    "split_file_and_save_parts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe3fbe7-ab9c-43fe-a5ff-08bc2bfb33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;32mCsvGeneratorDockerfile\u001b[00m\r\n",
      "├── \u001b[01;32mDockerpyspark\u001b[00m\r\n",
      "├── \u001b[01;32mDockerpysparksubmit\u001b[00m\r\n",
      "├── \u001b[01;32mREADME.md\u001b[00m\r\n",
      "├── \u001b[01;32mTestODBCConnections.ipynb\u001b[00m\r\n",
      "├── \u001b[01;32mUntitled.ipynb\u001b[00m\r\n",
      "├── \u001b[01;34martifacts\u001b[00m\r\n",
      "│   ├── \u001b[01;34mimages\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mbash.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mdependencies.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mdocker.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mpython.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mpython_generation_process_recap.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mtime_comparison_sequence.png\u001b[00m\r\n",
      "│   │   ├── \u001b[01;32mtime_results.png\u001b[00m\r\n",
      "│   │   └── \u001b[01;32mununtu.png\u001b[00m\r\n",
      "│   └── \u001b[01;32mpython.png\u001b[00m\r\n",
      "├── \u001b[01;32mcheck_deps.sh\u001b[00m\r\n",
      "├── \u001b[01;32mclear_all_docker_imgs.sh\u001b[00m\r\n",
      "├── \u001b[01;34mcluster_setup\u001b[00m\r\n",
      "│   ├── \u001b[01;32menv.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32mexternal.zip\u001b[00m\r\n",
      "│   ├── \u001b[01;32mget_secret.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32mhosts\u001b[00m\r\n",
      "│   ├── \u001b[01;32mlibraries.zip\u001b[00m\r\n",
      "│   ├── \u001b[01;32mpyspark_requirements.txt\u001b[00m\r\n",
      "│   ├── \u001b[01;32msetup_master_with_ufw.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32msetup_spark_playbook.yml\u001b[00m\r\n",
      "│   ├── \u001b[01;32mspark_submit_cluster.py\u001b[00m\r\n",
      "│   ├── \u001b[01;32msparksubmit.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32muninstall_spark_master_and_nodes.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32muninstall_spark_playbook.yml\u001b[00m\r\n",
      "│   ├── \u001b[01;32mworker1.ssh\u001b[00m\r\n",
      "│   └── \u001b[01;32mworker2.ssh\u001b[00m\r\n",
      "├── \u001b[01;32mcommand.txt\u001b[00m\r\n",
      "├── \u001b[01;32mcsvgenerator.sh\u001b[00m\r\n",
      "├── \u001b[01;34mdeploys\u001b[00m\r\n",
      "│   ├── \u001b[01;32mapply-deploys.sh\u001b[00m\r\n",
      "│   ├── \u001b[01;32mnamespace-pyspark-work.yaml\u001b[00m\r\n",
      "│   ├── \u001b[01;32mpyspark-work-spark-jn.yml\u001b[00m\r\n",
      "│   ├── \u001b[01;32mremove-deploys.sh\u001b[00m\r\n",
      "│   └── \u001b[01;34mtemplates\u001b[00m\r\n",
      "│       └── \u001b[01;32mpyspark-work-spark-jn.yml\u001b[00m\r\n",
      "├── \u001b[01;34mdot_net_csvgen\u001b[00m\r\n",
      "│   ├── \u001b[01;32mCsvGenerator.csproj\u001b[00m\r\n",
      "│   ├── \u001b[01;32mCsvGeneratorDockerfile\u001b[00m\r\n",
      "│   └── \u001b[01;32mProgram.cs\u001b[00m\r\n",
      "├── \u001b[01;34mexternal\u001b[00m\r\n",
      "│   └── \u001b[01;34mdb_and_excel_utilities\u001b[00m\r\n",
      "│       ├── \u001b[01;32mREADME.md\u001b[00m\r\n",
      "│       ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│       │   ├── \u001b[01;32mdb_base.cpython-39.pyc\u001b[00m\r\n",
      "│       │   └── \u001b[01;32mutility.cpython-39.pyc\u001b[00m\r\n",
      "│       ├── \u001b[01;32mcustom_excel_output.py\u001b[00m\r\n",
      "│       ├── \u001b[01;32mdb_base.py\u001b[00m\r\n",
      "│       ├── \u001b[01;32mdb_ins_fake_data.py\u001b[00m\r\n",
      "│       ├── \u001b[01;32mexcel_output.py\u001b[00m\r\n",
      "│       └── \u001b[01;32mutility.py\u001b[00m\r\n",
      "├── \u001b[01;32mgen_random_data.py\u001b[00m\r\n",
      "├── \u001b[01;32mget_utils.sh\u001b[00m\r\n",
      "├── \u001b[01;32mjava_opts.txt\u001b[00m\r\n",
      "├── \u001b[01;36mlibraries\u001b[00m -> \u001b[01;34mexternal/db_and_excel_utilities\u001b[00m\r\n",
      "├── \u001b[01;32mlinux_dependencies.txt\u001b[00m\r\n",
      "├── \u001b[01;32mmysql-connector-j_8.2.0-1ubuntu22.04_all.deb\u001b[00m\r\n",
      "├── \u001b[01;32mmysql-connector-j_8.2.0-1ubuntu22.04_all_repacked.deb\u001b[00m\r\n",
      "├── \u001b[01;32mpod-definition.yml\u001b[00m\r\n",
      "├── \u001b[01;32mpod-template.template\u001b[00m\r\n",
      "├── \u001b[01;32mpyspark_requirements.txt\u001b[00m\r\n",
      "├── \u001b[01;32mpysparkdocker_jn.sh\u001b[00m\r\n",
      "├── \u001b[01;32mpysparkdocker_jn1.sh\u001b[00m\r\n",
      "├── \u001b[01;32mpysparkmk8s_jn.sh\u001b[00m\r\n",
      "├── \u001b[01;32mreadme.txt\u001b[00m\r\n",
      "├── \u001b[01;32msetup_cluster_and_run_spark_job.sh\u001b[00m\r\n",
      "├── \u001b[01;32msetup_env.sh\u001b[00m\r\n",
      "├── \u001b[01;32mspark_submit.py\u001b[00m\r\n",
      "├── \u001b[01;32mspark_submit_cluster.py\u001b[00m\r\n",
      "├── \u001b[01;32msparksubmit.sh\u001b[00m\r\n",
      "├── \u001b[01;32mstart.sh\u001b[00m\r\n",
      "├── \u001b[01;32mswitchdc.sh\u001b[00m\r\n",
      "└── \u001b[01;32mtime_comparisons.sh\u001b[00m\r\n",
      "\r\n",
      "10 directories, 70 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
