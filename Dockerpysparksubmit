# Start from OpenJDK base image
FROM openjdk:11-jre-slim

# Install Python
RUN apt-get update && \
    apt-get install -y python3 python3-pip wget tree unixodbc unixodbc-dev && \
    apt-get clean;

# Set environment variables for Spark
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=/tmp/spark-events"
ENV SPARK_NO_DAEMONIZE=TRUE

RUN mkdir -p /tmp/spark-events
RUN chmod -R 777 /tmp/spark-events


# Download and install Spark
RUN wget -q https://downloads.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    tar xzf spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    mv spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /spark && \
    rm spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz

# Install PySpark and Jupyter
RUN pip3 install pyspark==$SPARK_VERSION jupyter

# Expose the port Jupyter will run on
EXPOSE 8888
EXPOSE 4040
EXPOSE 18080

# Set the working directory in the container
WORKDIR /workspace
COPY /pyspark_requirements.txt .
RUN pip install -r pyspark_requirements.txt

COPY start.sh /start.sh
RUN chmod +x /start.sh

CMD ["/bin/bash", "/start.sh"]

#/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
# Start Jupyter Notebook as the default command
#CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root", "--NotebookApp.token=''", "--NotebookApp.password=''"]

